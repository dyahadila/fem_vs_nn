{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI-kfxwkxWJv"
   },
   "source": [
    "# Solution of Linear PDEs in High Dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbqiIgtxWJ0"
   },
   "source": [
    "In this Jupyter Notebook we illustrate a PDE solver based on the Feynman-Kac formula as proposed in\n",
    "\n",
    "- Beck, Christian, et al. *Solving stochastic differential equations and Kolmogorov equations by means of deep learning*. [arXiv 1806.00421](https://arxiv.org/abs/1806.00421).\n",
    "\n",
    "This notebook is partially based on the TensorFlow 1 code appended to the aforementioned preprint.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/janblechschmidt/PDEsByNNs/blob/main/Feynman_Kac_Solver.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxwN1C_kxWJ1"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We consider the solution by neural network methods of a class of partial differential equations which arise as the *backward Kolmogorov equation* of stochastic processes known as *Itô diffusions*.\n",
    "We begin with linear parabolic second-order partial differential equation in non-divergence form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\partial_t u(t,x) + \\frac{1}{2} \\sigma \\sigma^T(t,x) : \\nabla^2 u(t,x) + \\mu(t,x) \\cdot \\nabla u(t,x) \n",
    "    &= 0, \t\t\t\\quad && (t,x) \\in [0,T) \\times \\mathbb R^d,\\\\\n",
    "    u(T,x) &= g(x), \t\\quad && x \\in \\mathbb R^d,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and subsequently move to more general PDEs.\n",
    "We consider the pure Cauchy problem, allowing the state variable $x$ to vary throughout $\\mathbb R^d$, were $d \\in \\mathbb{N}$ is the spatial dimension, \n",
    "$\\nabla u(t,x)$ and $\\nabla^2 u(t,x)$ denote the gradient and Hessian of the function $u$, respectively, the colon $:$ denotes the Frobenius inner product of $d \\times d$ matrices, i.e., $A:B = \\sum_{i,j=1}^d a_{ij} \\, b_{ij}$, and the dot $\\cdot$ the Euclidean inner product on $\\mathbb R^d$.\n",
    "We assume the coefficient functions $\\mu\\colon[0,T] \\times \\mathbb R^d \\to \\mathbb R^d$ (drift) and $\\sigma\\colon[0,T] \\times\\mathbb R^d \\to \\mathbb R^{d \\times d}$ (diffusion) to be globally Lipschitz continuous.\n",
    "Due to the stochastic process connection, the backward Kolmogorov equation is posed as a *final time problem* with data prescribed at time $t=T$ given by a function $g\\colon \\mathbb R^d \\to \\mathbb R$.\n",
    "The simple change of variables $t \\mapsto T - t$ leads to the more familiar initial value form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\partial_t u(t,x) - \\frac{1}{2} \\sigma \\sigma^T(t,x) : \\nabla^2 u(t,x) - \\mu(t,x) \\cdot \\nabla u(t,x) \n",
    "   &= 0, \t\t\t\\quad && (t,x) \\in (0,T] \\times \\mathbb R^d,\\\\\n",
    "   u(0,x) &= g(x), \t\\quad && x \\in\\mathbb R^d.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Equations in non-divergence form like the backward Kolmogorov equation with  leading term $\\sigma \\sigma^T(t,x) \\colon \\nabla^2u(t,x)$ typically arise in the context of stochastic differential equations (SDEs) due to the Itô formula.\n",
    "\n",
    "The goal of the computations that follow is to approximate the solution of the PDE at a fixed time $t=0$, i.e., the function $u(0,x)$ where $x$ varies over some $d$-dimensional hypercube $\\mathcal{D} := [a,b] \\subset \\mathbb{R}^d$ with endpoint vectors $a \\le b$ (component-wise). We denote this approximation by $u_\\theta(x)$, where $\\theta$ denotes a vector of parameters describing the degrees of freedom of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfq3CVDQxWJ1"
   },
   "source": [
    "## Theoretical Background\n",
    "The method is based on the Feynman-Kac formula for Kolmogorov PDEs which connects such PDEs with the expectation of a stochastic process.\n",
    "\n",
    "Given a filtered probability space $(\\Omega, \\mathcal{F}, \\mathbb{P}; \\mathbb{F})$ equipped with the filtration $\\mathbb{F}=\\{\\mathcal{F}_t\\}_{t\\in[0,T]}$ induced by a Brownian motion $\\{W_t\\}_{t \\in [0,T]}$,\n",
    "the stochastic process $\\{X_t\\}_{t \\in [0,T]}$ governed by the SDE \n",
    "\n",
    "$$\n",
    "X_t = x + \\int_0^t \\mu(s, X_s) \\, ds + \\int_0^t \\sigma(s, X_s) \\, dW_s,\n",
    "\\qquad t \\in [0,T]\n",
    "$$\n",
    "\n",
    "describes the evolution of the state variable in $\\mathbb R^d$. The Feynman-Kac formula connects the solution of the (linear) PDE given above with this stochastic process.\n",
    "Specifically, its solution at a point $(t,x) \\in [0,T] \\times \\mathbb R^d$ is given by the conditional expectation\n",
    "\n",
    "$$\n",
    "u(t,x) = \\mathbb{E} [g(X_T) | X_t = x].\n",
    "$$\n",
    "\n",
    "This connection can be exploited to approximate the PDE solution at a fixed point in space and time $(t,x)$ by simulating many paths/realizations of the stochastic process\n",
    "$\\{X^j_\\tau\\}_{\\tau \\in [t,T]}$, which all start at $X_t=x$\n",
    "for $j=1,\\ldots,n_{samples}$ and approximating the expectation by the sample average\n",
    "\n",
    "$$\n",
    "u(t,x) \\approx \\frac{1}{n_{samples}} \\sum_{j=1}^{n_{samples}} g(X_T^j).\n",
    "$$\n",
    "\n",
    "The method reviewed here differs in that it constructs, for a fixed time $t_0$ (we set $t_0=0$ below), an approximation of the solution not only at a single point $(t_0,x) \\in [0,T] \\times \\mathbb{R}^d$ but throughout a bounded domain of interest $\\mathcal{D} \\subset \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kh4I9xXvRMc"
   },
   "source": [
    "## Solution Method\n",
    "The approach relies on\n",
    "\n",
    "1. the generation of training data by generating sample paths of the stochastic process $\\{X_t\\}_{t \\in [0,T]}$ and\n",
    "2. a neural network approximation of the unknown function $u(0,x) \\approx u_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdx4mFItGjCG"
   },
   "source": [
    "### Step 1: Generation of training data\n",
    "\n",
    "The neural network model is trained with data pairs $(x, y)$ such that the values of $x$ are realizations of a random variable $X \\sim \\mathsf U (\\mathcal D)$ and $y = g(X_T)$, where $X_T$ is the final value at $t=T$ of the stochastic process $\\{X_t\\}_{t \\in[0,T]}$ evolving according to the SDE\n",
    "\n",
    "$$\n",
    "X_T = x + \\int_0^T \\mu(s, X_s) \\, ds + \\int_0^T \\sigma(s, X_s) \\, dW_s.\n",
    "$$\n",
    "\n",
    "Each path of the process starts at a uniformly drawn initial point $X_0 = x$.\n",
    "We distinguish two cases:\n",
    "\n",
    "#### Simple case: Distribution of $X_T$ explicitly known\n",
    "\n",
    "When the probability distribution of $X_T$ is explicitly known, we can draw pairs $(x,y)$ directly.\n",
    "For example, when $\\{X_t\\}$ is a scaled Brownian motion with dynamics characterized by $\\mu(t,x) \\equiv 0$ and $\\sigma(t,x) \\equiv \\sigma$, the solution process is given by\n",
    "\n",
    "$$\n",
    "X_t = x + \\sigma \\, W_t, \\qquad 0 \\le t \\le T.\n",
    "$$\n",
    "\n",
    "Thus, we may draw $x$ from $\\mathsf U (\\mathcal D)$ and set $y := g(x + \\sqrt{T} \\, \\sigma \\, \\xi)$ where $\\xi \\sim \\mathsf N (0, \\mathrm{Id}_{d \\times d})$ is a random variable with a $d$-variate standard normal distribution.\n",
    "\n",
    "#### General case: Distribution of $X_T$ unknown\n",
    "\n",
    "When the solution process is not available in closed form, we can still generate approximate sample paths of $\\{X_t\\}_{t \\in [0,T]}$ numerically in order to sample from $X_T$.\n",
    "This can be done, for example, using the Euler-Maruyama scheme:\n",
    "\n",
    "$$\n",
    "\\tilde X_{n+1} := \\tilde X_{n}\n",
    "    + \\mu(t_n, \\tilde X_{n}) \\, (t_{n+1} - t_n)\n",
    "    + \\sigma(t_n, \\tilde X_{n}) \\, (W_{t_{n+1}} - W_{t_n}),\n",
    "\\qquad\n",
    "\\tilde X_0 := x,\n",
    "$$\n",
    "\n",
    "where $\\tilde X_n \\approx  X_{t_n}$ and $0=t_0 < t_1 < \\ldots < t_N = T$ is a discretization of the time domain.\n",
    "Note that the increment of a Brownian motion $(W_{t_{n+1}} - W_{t_n}) \\sim \\mathsf{N}(0,(t_{n+1} - t_n) I_{d\\times d})$ is normally distributed.\n",
    "Finally, we set $y := g(\\tilde X_N)$.\n",
    "\n",
    "*Note*: Strong convergence results for the Euler-Maruyama scheme (cf. [Kloeden, Platen (1992)](https://www.springer.com/de/book/9783540540625)) ensure that $\\tilde X_n \\to X_{t_n}$ pathwise as $N \\to \\infty$ and $\\sup |t_n - t_{n-1}| \\to 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sdvYFcXGs8h"
   },
   "source": [
    "### Step 2: Neural network approximation\n",
    "The PDE solution $u \\colon [0,T] \\times \\mathcal D \\to \\mathbb R$ is now approximated by neural networks with structure as given in [Beck et al. (2018)](https://arxiv.org/abs/1806.00421) as\n",
    "\n",
    "    Input -> BN -> (Dense -> BN -> TanH) -> (Dense -> BN -> TanH) -> Dense -> BN -> Output\n",
    "    \n",
    "Here, `BN` denotes Batch Normalization, `Dense` a fully connected layer **without** bias term and activation and `TanH` the application of the componentwise hyperbolic tangent activation function\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYfwElhSxWJ3"
   },
   "source": [
    "## Example: Heat equation\n",
    "\n",
    "As an example, consider the Cauchy problem for the $d$-dimensional heat equation\n",
    "\n",
    "$$\n",
    "\\partial_t u(t,x) = \\Delta u (t,x), \\qquad\n",
    "u(0,x) = \\|x\\|^2,\n",
    "$$\n",
    "where $\\Delta u = \\sum_{i=1}^d \\partial^2 u/\\partial x_i^2$ denotes the Laplacian of $u$.\n",
    "\n",
    "The problem is posed as an initial value problem with drift coefficient $\\mu(t,x) \\equiv 0$ and diffusion coefficient $\\sigma(t,x) \\equiv \\sqrt 2 $. One easily verifies that the solution is given by\n",
    "\n",
    "$$\n",
    "u(t,x) = \\|x\\|^2 + 2 \\, t \\, d.\n",
    "$$\n",
    "\n",
    "Our domain of interest is chosen as the $d$-dimensional unit cube $\\mathcal D = [0,1]^d$.\n",
    "\n",
    "In the following, we pesent code fragments for solving the heat equation in 100 spatial dimensions. \n",
    "The running time is reduced significantly by GPU acceleration, such as is available when running this Jupyter notebook on Google Colab (roughly 1 hour). Remember to explicitly enable GPU mode in the settings.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/janblechschmidt/PDEsByNNs/blob/main/Feynman_Kac_Solver.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB5d-AayxWJ4"
   },
   "source": [
    "### 1. Initialize problem\n",
    "This code runs with current TensorFlow versions (tested with 2.3.0 and 2.4.1).\n",
    "First, we import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0JdyLWkxWJ4",
    "outputId": "2b91a57a-f477-4655-9e07-9bf288348816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version used: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from time import time\n",
    "\n",
    "# Set data type\n",
    "DTYPE='float32'\n",
    "#DTYPE='float64'\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "print('TensorFlow version used: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki-CS2bqNCh-"
   },
   "source": [
    "Next, we set some problem specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v7JbleagxWJ6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 14:04:39.719163: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Final time\n",
    "T = tf.constant(1., dtype=DTYPE)\n",
    "\n",
    "# Spatial dimension\n",
    "dim = 100\n",
    "\n",
    "# Spatial domain of interest at t=0 (hyperrectangle)\n",
    "a = np.zeros((dim), dtype=DTYPE)\n",
    "b = np.ones((dim), dtype=DTYPE)\n",
    "\n",
    "# Diffusion coefficient is assumed to be constant\n",
    "sigma = np.sqrt(2, dtype=DTYPE)\n",
    "\n",
    "# Define initial time condition\n",
    "def fun_g(x):\n",
    "    return tf.reduce_sum(tf.pow(x,2), axis=1, keepdims=True)\n",
    "\n",
    "# Define exact reference solution \n",
    "def fun_u(t, x):\n",
    "    return tf.reduce_sum(tf.pow(x,2), axis=1, keepdims=True) + 2 * t * dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ved8NAK4PBtY"
   },
   "source": [
    "### 2. Generation of training data\n",
    "Next, we define a method to generate `num_samples` of training data $(x, y)$.\n",
    "We are in the setting where we know the distribution of $X_T$. \n",
    "Thus, it suffices to\n",
    "- draw $x$ uniformly from $\\mathcal D$, i.e.,\n",
    "$$X \\sim \\mathsf U(\\mathcal D),$$\n",
    "- set\n",
    "$$y := g(X + \\sigma \\, \\sqrt{T} \\,\\xi),$$\\\n",
    "where $\\xi \\sim \\mathsf N(0, \\mathrm{Id}_{d \\times d})$ is drawn from a $d$-variate standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D2foRCyXQoUY"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def draw_X(num_samples, a, b, sigma=1.0):\n",
    "    \"\"\" Function to draw num_samples pairs of starting values X_0 \n",
    "    and end points X_T of a stochastic process X with zero drift and\n",
    "    constant scalar diffusion. Starting points are drawn uniformly from the\n",
    "    hypercube [a,b] \\\\subset \\mathbb{R}^d. \"\"\"\n",
    "    dim = a.shape[0]\n",
    "    \n",
    "    X0 = a + tf.random.uniform((num_samples, dim), dtype=DTYPE) * (b-a)\n",
    "\n",
    "    # Initialize the array X\n",
    "    xi = tf.random.normal(shape=(num_samples, dim), dtype=DTYPE)\n",
    "    \n",
    "    XT = X0 + sigma * np.sqrt(T) * xi\n",
    "\n",
    "    # Return simulated paths as well as increments of Brownian motion\n",
    "    return tf.stack([X0, XT], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzkSIzzgT9zK"
   },
   "source": [
    "### 3. Set up neural network model\n",
    "\n",
    "The next function returns a neural network model in which one may change the used activation function (default: $\\tanh$) as well as the number of hidden neurons (default: $n_{hidden}=100$) in each of the hidden layers.\n",
    "\n",
    "    Input -> BN -> (Dense -> BN -> TanH) -> (Dense -> BN -> TanH) -> Dense -> BN -> Output\n",
    "    \n",
    "Here, `BN` denotes Batch Normalization, `Dense` indicates a fully connected layer **without** bias term and activation, `TanH` refers to the componentwise application of the hyperbolic tangent activation function.\n",
    "\n",
    "The chosen model contains a total of $11\\,944$ unknown parameters allotted to the network components as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underbrace{4\\cdot d}_{\\texttt{BN0}}\n",
    "+ \\underbrace{d\\cdot n}_{\\texttt{Dense1}}\n",
    "+ \\underbrace{4\\cdot n}_{\\texttt{BN1}}\n",
    "+ \\underbrace{n\\cdot n}_{\\texttt{Dense2}}\n",
    "+ \\underbrace{4\\cdot n}_{\\texttt{BN2}} &\n",
    "+ \\underbrace{n\\cdot 1}_{\\texttt{Dense3}}\n",
    "+ \\underbrace{4\\cdot 1}_{\\texttt{BN3}}\n",
    "\\\\\n",
    "&\\quad= 4\\,(d + 1) + n\\,(d+9) + n^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-UHnKtyvxWJ8"
   },
   "outputs": [],
   "source": [
    "def init_model(dim, activation='tanh',\n",
    "               num_hidden_neurons=100,\n",
    "               num_hidden_layers=2,\n",
    "               initializer=tf.keras.initializers.GlorotUniform()):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(dim))\n",
    "    model.add(tf.keras.layers.BatchNormalization(epsilon=1e-6))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_hidden_neurons,\n",
    "                                activation=None,\n",
    "                                use_bias=False,\n",
    "                                kernel_initializer=initializer\n",
    "                                ))\n",
    "        model.add(tf.keras.layers.BatchNormalization(epsilon=1e-6))\n",
    "        model.add(tf.keras.layers.Activation(activation))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1,\n",
    "                                    activation=None,\n",
    "                                    use_bias=False,\n",
    "                                    kernel_initializer=initializer\n",
    "                                    ))\n",
    "    model.add(tf.keras.layers.BatchNormalization(epsilon=1e-6))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzNT0vhQxWJ9"
   },
   "source": [
    "### 4. Define functions for the objective and its gradient\n",
    "In the next step, we define the loss function, i.e., the function to be minimized in the neural network's training.\n",
    "To achieve $u_\\theta(T,x) \\approx u(T,x) = \\mathbb{E}[g(X_T) | X_0=x]$, we minimize the mean-square error (MSE)\n",
    "\n",
    "$$\n",
    "\\mathbb{E} [ |g(X_T) - u_\\theta(T,x)|^2 ],\n",
    "$$\n",
    "\n",
    "where the expectation is taken with respect to the joint distribution of initial values and sample paths.\n",
    "The existence of a unique function $u$ which minimizes this objective and solves the PDE is based on Prop. 2.7 of [Beck et al. (2018)](https://arxiv.org/abs/1806.00421).\n",
    "\n",
    "For batches of size $m$, we approximate this quantity by the sample mean\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m |y^i - u_\\theta(T,x^i)|^2,\n",
    "$$\n",
    "\n",
    "where $x^i$ is drawn uniformly from ${\\mathsf U}(\\mathcal D)$ and $y^i$ is a realization of $g(X_T^i)$ where $X_0^i = x^i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RrjDa9XFxWJ9"
   },
   "outputs": [],
   "source": [
    "def loss_fn(X, y, model, training=False):\n",
    "    \"\"\" This function computes the mean squared error between the current model\n",
    "    prediction model(X) and the values y.\n",
    "    Inputs:\n",
    "        X - approximation to the state process X\n",
    "        y - target value\n",
    "        model - model of neural network approximating x -> u(T,x)\n",
    "        training - boolean flag to indicate training\n",
    "    \"\"\"\n",
    "\n",
    "    X0 = X[:,:,0]\n",
    "    y_pred = model(X0, training)\n",
    "    \n",
    "    # Return mean squared error\n",
    "    return tf.reduce_mean(tf.math.squared_difference(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7tGl-0MxWJ9"
   },
   "source": [
    "The next step uses the automatic differentiation functionality of TensorFlow to compute the gradient of the loss function with respect to the unknowns $\\theta$, called `trainable_variables` in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CRM5hJvjxWJ-"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_grad(X, y, model, training=False):\n",
    "    \"\"\" This function computes the gradient of the loss function w.r.t.\n",
    "    the trainable variables theta.\n",
    "    Inputs:\n",
    "        X - approximation to the state process X\n",
    "        y - target value\n",
    "        model - model of neural network approximating x -> u(T,x)\n",
    "        training - boolean flag to indicate training\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(X, y, model, training)\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4b957hTxWJ_"
   },
   "source": [
    "### 5. Solve the PDE\n",
    "\n",
    "Next, we initialize the model and choose an optimizer as well as a learning rate schedule.\n",
    "\n",
    "The optimizer which performs the gradient descent step and in particular its step size (aka learning rate) have to be chosen carefully.\n",
    "The speed of convergence can depend strongly on the initialization of the unknown network parameters $\\theta$ as well as the optimizer settings, e.g., choosing the learning rate too high may yield a divergent sequence while values which are too low may result in slow convergence.\n",
    "\n",
    "In addition, we define a test data set $(X_{test}, Y_{test})$ consisting of uniformly drawn points $x^i_{test} \\subset \\mathcal D$ and target values $y^i_{test} = u(T,x^i_{test}).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O2DmiMmTE4Xl"
   },
   "outputs": [],
   "source": [
    "# Original experiment\n",
    "model = init_model(dim=dim, num_hidden_neurons=200)\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([250001,500001],[1e-3,1e-4,1e-5])\n",
    "suffix='orig'\n",
    "\n",
    "# Exponential decay\n",
    "#model = init_model(dim=dim, num_hidden_neurons=200)\n",
    "#lr = tf.keras.optimizers.schedules.ExponentialDecay(1e-1,100000,.1)\n",
    "#suffix='exp'\n",
    "\n",
    "# More layers and neurons experiment\n",
    "#model = init_model(dim=dim, num_hidden_neurons=300, num_hidden_layers=3)\n",
    "#lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([250001,500001],[1e-3,1e-4,1e-5])\n",
    "#suffix='more'\n",
    "\n",
    "# Even more\n",
    "#model = init_model(dim=dim, num_hidden_neurons=400, num_hidden_layers=4)\n",
    "#lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([250001,500001],[1e-3,1e-4,1e-5])\n",
    "#suffix='even_more'\n",
    "\n",
    "# Deep ReLU Networks\n",
    "#model = init_model(dim=dim, num_hidden_neurons=200, num_hidden_layers=4, initializer=tf.keras.initializers.HeNormal())\n",
    "#lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([250001,500001],[1e-3,1e-4,1e-5])\n",
    "#suffix='relu'\n",
    "\n",
    "# Set number of training epochs\n",
    "num_epochs = 750001\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 8192\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "\n",
    "# Initialize list containing history of losses\n",
    "hist_loss = []\n",
    "error_hist = []\n",
    "\n",
    "# Randomly choose a test set from \\mathcal{D} to approximate\n",
    "# errors by means of Monte Carlo sampling\n",
    "n_test = 1000000\n",
    "X = draw_X(n_test, a, b, sigma=sigma)\n",
    "Xtest = X[:,:,0]\n",
    "# Compute exact solution\n",
    "Ytest = fun_u(T,Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHGGkULeT6UZ"
   },
   "source": [
    "Next, we train the model.\n",
    "Here, we decided to make this explicit in order to show what happens in detail.\n",
    "Note, however, that one might implement a `Python generator` or `tf.keras.utils.Sequential` object to provide the training data and use the `fit()` method from the `tf.keras.Model` object to train the model.\n",
    "\n",
    "**Note**: The following code cell has a runtime of about 60 minutes on Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9-doxyhDxJY",
    "outputId": "4190d338-419c-46c7-93a0-5471c1be2889",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a training step as a TensorFlow function to increase speed of training\n",
    "@tf.function\n",
    "def train_step():\n",
    "    # Draw batch of random paths\n",
    "    X = draw_X(batch_size, a, b, sigma=sigma)\n",
    "    \n",
    "    # Evaluate g at X_T\n",
    "    y = fun_g(X[:,:,-1])\n",
    "\n",
    "    # And compute the loss as well as the gradient\n",
    "    loss, grad = compute_grad(X, y, model, training=True)\n",
    "\n",
    "    # Perform gradient step\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Start timer\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "# Set interval to estimate errors\n",
    "log_interval = 1000\n",
    "\n",
    "\n",
    "# Initialize header of output\n",
    "print('  Iter        Loss   L1_rel   L2_rel   Linf_rel |   L1_abs   L2_abs   Linf_abs  |    Time  Stepsize')\n",
    "\n",
    "# Loop to train model\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # Perform training step\n",
    "    loss = train_step()\n",
    "    hist_loss.append(loss)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        \n",
    "        # Compute current prediction on test set\n",
    "        Ypred = model(Xtest, training=False)\n",
    "        \n",
    "        # Compute absolute and relative errors\n",
    "        abs_error = np.abs(Ypred - Ytest)\n",
    "        rel_error = abs_error/Ytest\n",
    "        L2_rel = tf.sqrt(tf.reduce_mean(tf.pow(rel_error, 2))).numpy()\n",
    "        L1_rel = tf.reduce_mean(tf.abs(rel_error)).numpy()\n",
    "        Linf_rel = tf.reduce_max(tf.abs(rel_error)).numpy()\n",
    "        \n",
    "        L2_abs = tf.sqrt(tf.reduce_mean(tf.pow(abs_error, 2))).numpy()\n",
    "        L1_abs = tf.reduce_mean(tf.abs(abs_error)).numpy()\n",
    "        Linf_abs = tf.reduce_max(tf.abs(abs_error)).numpy()\n",
    "        total_time = time()-t0\n",
    "        stepsize = optimizer.lr(optimizer.iterations).numpy()\n",
    "        err = (i, loss.numpy(), L1_rel, L2_rel, Linf_rel, L1_abs, L2_abs, Linf_abs, total_time, stepsize)\n",
    "        error_hist.append(err)\n",
    "        print('{:5d} {:12.4f} {:8.4f} {:8.4f}   {:8.4f} | {:8.4f} {:8.4f}   {:8.4f}  |  {:6.1f}  {:6.2e}'.format(*err))\n",
    "print(time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlT3PuBVxWKC"
   },
   "source": [
    "### Plot of the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "MIKk28sRxWKC",
    "outputId": "d30ca45e-ab9e-4d23-d6c8-6eb1b75f7d66"
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "ax.semilogy(range(len(hist_loss)), hist_loss,'k-')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('training loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSUsWDowpgM1"
   },
   "source": [
    "### Plot of the absolute and relative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "ToR6G-zRoceu",
    "outputId": "e025120e-1dc8-4c64-d617-a6eee87e6f01"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(9,6))\n",
    "ax.set_prop_cycle(color=['royalblue', 'darkorange', 'darkcyan'])\n",
    "start_idx = 0\n",
    "xrange = [e[0] for e in error_hist[start_idx:]]\n",
    "ax.semilogy(xrange, [e[2:5] for e in error_hist[start_idx:]])\n",
    "ax.semilogy(xrange, [e[5:8] for e in error_hist[start_idx:]], linestyle='--')\n",
    "ax.set_xlabel('$n_{epoch}$')\n",
    "ax.set_ylabel('Errors')\n",
    "ax.legend(['$L^1$', '$L^2$', '$L^{\\infty}$']),\n",
    "#plt.savefig('Errors_HeatEquation_dim_{:03d}_{:s}.pdf'.format(dim,suffix), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi_X89POUGwN"
   },
   "outputs": [],
   "source": [
    "SAVE_CSV = False\n",
    "if SAVE_CSV:\n",
    "    import csv\n",
    "    with open('heat_dim_{:03d}_{:s}.csv'.format(dim, suffix),'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['Iter', 'Loss', 'L1_rel', 'L2_rel', 'Linf_rel', 'L1_abs', 'L2_abs', 'Linf_abs', 'Time', 'Stepsize'])\n",
    "        for row in error_hist:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m9vYreOKN0p"
   },
   "source": [
    "### Plot slice of solution and rel. error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "Do1Sfm8zxWKC",
    "outputId": "f4c6e344-7bdb-453f-ac8b-826d965cac69"
   },
   "outputs": [],
   "source": [
    "ngrid = 400\n",
    "\n",
    "# Choose indices to plot\n",
    "idx0 = 0\n",
    "idx1 = 1\n",
    "\n",
    "# Determine remaining indices\n",
    "idx = np.arange(dim)\n",
    "idx_remaining = np.setdiff1d(idx,[idx0,idx1])\n",
    "# Set remaining values to midpoints\n",
    "val_remaining = (a+b)/2\n",
    "\n",
    "# Meshgrid for plots\n",
    "xspace = np.linspace(a[idx0], b[idx0], ngrid + 1, dtype=DTYPE)\n",
    "yspace = np.linspace(a[idx1], b[idx1], ngrid + 1, dtype=DTYPE)\n",
    "X,Y = np.meshgrid(xspace, yspace)\n",
    "\n",
    "# Append remaining values\n",
    "Z = np.repeat(val_remaining[idx_remaining].reshape(1,-1), (ngrid+1)**2,axis=0)\n",
    "Xgrid = np.vstack([X.flatten(),Y.flatten()]).T\n",
    "Xgrid = np.hstack([Xgrid,Z])\n",
    "\n",
    "# Relative error\n",
    "urel =tf.abs(model(Xgrid,training=False) -  fun_u(T,Xgrid))/fun_u(T,Xgrid)\n",
    "Urel = urel.numpy().reshape(ngrid+1,ngrid+1)\n",
    "# Slice of solution\n",
    "U = model(Xgrid,training=False).numpy().reshape(ngrid+1,ngrid+1)\n",
    "\n",
    "# Plot solution of heat equation\n",
    "fig = plt.figure(figsize=(11,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X,Y,U,cmap='viridis')\n",
    "ax.view_init(45,210)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "#plt.savefig('Sol_HeatEquation_dim_{:03d}_{:s}.pdf'.format(dim,suffix), bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Plot relative error of heat equation\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X,Y,Urel,cmap='viridis')\n",
    "ax.view_init(45,210)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "#plt.savefig('RelError_HeatEquation_dim_{:03d}_{:s}.pdf'.format(dim,suffix), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXYmJ5A-QHKI"
   },
   "source": [
    "## Example: Option pricing\n",
    "In this example, we study the Black-Scholes formula, one of the basic models used in mathematical finance to determine the fair value of derivatives.\n",
    "\n",
    "We consider a financial market with 100 uncorrelated risky assets whose underlying dynamics is given by a geometric Brownian motion, i.e., a stochastic process $\\{X_t\\}_{t \\in [0,T]}$ governed by the SDE \n",
    "\n",
    "$$\n",
    "X_t = x + \\int_0^t \\mu(s,X_s) \\, ds + \\int_0^t \\sigma(s,X_s) \\, dW_s,\n",
    "\\qquad t \\in [0,T]\n",
    "$$\n",
    "\n",
    "with $\\mu(s,x) \\equiv -0.05 \\, x$ and $\\sigma(s,x) \\equiv \\mathrm{diag}(0.105 \\, x_1, 0.110 \\, x_2, \\ldots, 0.600 \\, x_{100})$.\n",
    "\n",
    "Here, we consider a derivative whose fair value at $(t,x)$ is determined by the PDE\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\partial_t u(t,x) - \\frac{1}{2} \\sum_{i=1}^d | \\sigma_i x_i |^2 \\, \\partial^2_{x_i} u(t,x) - \\sum_{i=1}^d \\mu_i x_i \\partial_{x_i} u(t,x)\n",
    "       &= 0, \t\t\t\\quad && (t,x) \\in (0,T] \\times \\mathbb R_+^d,\\\\\n",
    "   u(0,x) &= g(x), \t\\quad && x \\in\\mathbb R_+^d.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "$g(x) := \\exp (-r \\, T) \\max \\{\n",
    "[ \\max_{i\\in\\{1, \\ldots, 100\\}} x_i ] - 100, 0\n",
    "\\}.\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "The model is taken from [Beck et al., Sec. 3.3](https://arxiv.org/abs/1806.00421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py4fp7IZRZpI"
   },
   "outputs": [],
   "source": [
    "# Final time\n",
    "T = tf.constant(1., dtype=DTYPE)\n",
    "\n",
    "# Spatial dimensions\n",
    "dim = 100\n",
    "\n",
    "# Domain-of-interest at t=0\n",
    "a = 90 * tf.ones((dim), dtype=DTYPE)\n",
    "b = 110 * tf.ones((dim), dtype=DTYPE)\n",
    "\n",
    "# Interest rate\n",
    "r = tf.constant(1./20, dtype=DTYPE)\n",
    "\n",
    "# Drift\n",
    "mu = tf.constant(-1./20, dtype=DTYPE)\n",
    "\n",
    "# Strike price\n",
    "K = tf.constant(100., dtype=DTYPE)\n",
    "\n",
    "# Diffusion/volatility\n",
    "sigma = 1./10 + 1./200*tf.range(1, dim+1, dtype=DTYPE)\n",
    "\n",
    "# Define terminal condition, i.e., payoff at maturity\n",
    "def fun_g(x):\n",
    "    return tf.exp(-r*T) * tf.maximum(tf.reduce_max(x, axis=1, keepdims=True) - K, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOkzanumSla5"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def draw_X(num_samples, a, b):\n",
    "    \"\"\" Function to draw num_samples many pairs of uniformly drawn starting\n",
    "    values X_0 and end points X_T of a stochastic process X with zero drift and\n",
    "    constant scalar diffusion. Starting points are drawn uniformly from the\n",
    "    hypercube [a,b] \\\\subset \\mathbb{R}^d. \"\"\"\n",
    "    dim = a.shape[0]\n",
    "    \n",
    "    X0 = a + tf.random.uniform((num_samples, dim), dtype=DTYPE) * (b-a)\n",
    "    \n",
    "    # Initialize the array X\n",
    "    xi = tf.random.normal(shape=(num_samples, dim), dtype=DTYPE)\n",
    "    XT = X0 * tf.exp( (mu- tf.square(sigma)/2) * T + sigma * tf.sqrt(T) * xi)\n",
    "\n",
    "    # Return simulated paths as well as increments of Brownian motion\n",
    "    return tf.stack([X0, XT], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7jx6k-dKzD"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = init_model(dim=dim, num_hidden_neurons=dim+100)\n",
    "\n",
    "# Set up step size\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([250001,500001],[1e-3,1e-4,1e-5])\n",
    "\n",
    "# Set number of training epochs\n",
    "num_epochs = 750001\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 8192\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "\n",
    "# Initialize list containing history of losses\n",
    "hist_loss = []\n",
    "error_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4D2oAz5vJpR"
   },
   "source": [
    "Draw a test set with 250 000 samples and estimate the target values by means of Monte-Carlo estimation with $32 \\times 32 000$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O92cmffUvCt1",
    "outputId": "0b2e78a1-ffa4-4030-f2f1-dd0fbae2dd35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomly choose a test set from \\mathcal{D} to approximate\n",
    "# errors by means of Monte Carlo sampling\n",
    "n_test = 250000\n",
    "X = draw_X(n_test, a, b)\n",
    "Xtest = X[:,:,0]\n",
    "Xtest = tf.convert_to_tensor(Xtest, dtype=DTYPE)\n",
    "\n",
    "# Prepare Ytest\n",
    "Ytest = tf.zeros((Xtest.shape[0],1),dtype=DTYPE)\n",
    "\n",
    "# Determine values at Xtest through Monte-Carlo sampling\n",
    "# Set batch size and mc_samples, results in b_size*mc_samples many iterates\n",
    "b_size = 32\n",
    "mc_samples = 32000\n",
    "\n",
    "@tf.function\n",
    "def mc_step(y):\n",
    "    # Draw standard normal random variables\n",
    "    Xi = tf.random.normal(shape=(b_size, dim), dtype=DTYPE)\n",
    "    # Determine update\n",
    "    upd = tf.exp( (mu- tf.square(sigma)/2) * T + sigma * tf.sqrt(T) * Xi)\n",
    "    # Determine X_T\n",
    "    XT = tf.reshape(Xtest, shape=[n_test, dim, 1])*tf.transpose(upd)\n",
    "    return y + tf.reduce_sum(tf.reshape(fun_g(XT),[n_test, b_size]), axis=1,keepdims=True)/(b_size*mc_samples)\n",
    "\n",
    "for i in range(mc_samples):\n",
    "    if i%100==0:\n",
    "        print(i,'/',mc_samples)\n",
    "    Ytest=mc_step(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Janmi9aXUX44",
    "outputId": "fdff7fab-2f4c-4cdc-b6b9-6abc3e4a2c0b"
   },
   "outputs": [],
   "source": [
    "# Define a training step as a TensorFlow function to increase speed of training\n",
    "@tf.function\n",
    "def train_step():\n",
    "    # Draw batch of random paths\n",
    "    X = draw_X(batch_size, a, b)\n",
    "    \n",
    "    # Evaluate g at X_T\n",
    "    y = fun_g(X[:,:,-1])\n",
    "\n",
    "    # And compute the loss as well as the gradient\n",
    "    loss, grad = compute_grad(X, y, model, training=True)\n",
    "    #hist_loss.append(loss)\n",
    "\n",
    "    # Perform gradient step\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Start timer\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "# Set interval to estimate errors\n",
    "log_interval = 1000\n",
    "\n",
    "\n",
    "# Initialize header of output\n",
    "print('  Iter        Loss   L1_rel   L2_rel   Linf_rel |   L1_abs   L2_abs   Linf_abs  |    Time')\n",
    "\n",
    "# Loop to train model\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # Perform training step\n",
    "    loss = train_step()\n",
    "    hist_loss.append(loss)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        \n",
    "        # Compute current prediction on test set\n",
    "        Ypred = model(Xtest, training=False)\n",
    "        \n",
    "        # Compute absolute and relative errors\n",
    "        abs_error = np.abs(Ypred - Ytest)\n",
    "        rel_error = abs_error/Ytest\n",
    "        L2_rel = tf.sqrt(tf.reduce_mean(tf.pow(rel_error, 2))).numpy()\n",
    "        L1_rel = tf.reduce_mean(tf.abs(rel_error)).numpy()\n",
    "        Linf_rel = tf.reduce_max(tf.abs(rel_error)).numpy()\n",
    "        \n",
    "        L2_abs = tf.sqrt(tf.reduce_mean(tf.pow(abs_error, 2))).numpy()\n",
    "        L1_abs = tf.reduce_mean(tf.abs(abs_error)).numpy()\n",
    "        Linf_abs = tf.reduce_max(tf.abs(abs_error)).numpy()\n",
    "        \n",
    "        total_time = time()-t0\n",
    "        stepsize = optimizer.lr(optimizer.iterations)\n",
    "        err = (i, loss.numpy(), L1_rel, L2_rel, Linf_rel, L1_abs, L2_abs, Linf_abs, total_time, stepsize)\n",
    "        error_hist.append(err)\n",
    "        print('{:5d} {:12.4f} {:8.4f} {:8.4f}   {:8.4f} | {:8.4f} {:8.4f}   {:8.4f}  |  {:6.1f}  {:6.2e}'.format(*err))\n",
    "#Y = model.call((X,dw))\n",
    "print(time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4ZUlkFL0QMX"
   },
   "source": [
    "### Plot of the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "WlYZhvu50QMl",
    "outputId": "9ff4c99e-4730-47f4-e795-1ae120e367a6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "ax.semilogy(range(len(hist_loss)), hist_loss,'k-')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('training loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIUPehHg0QMm"
   },
   "source": [
    "### Plot of the absolute and relative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "ap_Vufdl0QMn",
    "outputId": "4ad0f7f2-8013-4265-f134-3e25124f4686"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(9,6))\n",
    "ax.set_prop_cycle(color=['royalblue', 'darkorange', 'darkcyan'])\n",
    "xrange = np.arange(len(error_hist))*log_interval\n",
    "ax.semilogy(xrange, [e[2:5] for e in error_hist])\n",
    "ax.semilogy(xrange, [e[5:8] for e in error_hist], linestyle='--')\n",
    "ax.set_xlabel('$n_{epoch}$')\n",
    "ax.set_ylabel('Errors')\n",
    "ax.legend(['$L^1$', '$L^2$', '$L^{\\infty}$']),\n",
    "plt.savefig('Errors_OptionPricing.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH4vzONY0QMn"
   },
   "source": [
    "### Plot slice of solution and rel. error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbjc_5TDkWUf",
    "outputId": "33511132-138f-48df-f0cf-a670123f1fcd"
   },
   "outputs": [],
   "source": [
    "ngrid = 100\n",
    "\n",
    "# Choose indices to plot\n",
    "idx0 = 0\n",
    "idx1 = 1\n",
    "\n",
    "# Determine remaining indices\n",
    "idx = np.arange(dim)\n",
    "idx_remaining = np.setdiff1d(idx,[idx0,idx1])\n",
    "# Set remaining values to midpoints\n",
    "val_remaining = np.array((a+b)/2)\n",
    "\n",
    "# Meshgrid for plots\n",
    "xspace = np.linspace(a[idx0], b[idx0], ngrid + 1, dtype=DTYPE)\n",
    "yspace = np.linspace(a[idx1], b[idx1], ngrid + 1, dtype=DTYPE)\n",
    "X,Y = np.meshgrid(xspace, yspace)\n",
    "\n",
    "# Append remaining values\n",
    "Z = np.repeat(val_remaining[idx_remaining].reshape(1,-1), (ngrid+1)**2,axis=0)\n",
    "Xgrid = np.vstack([X.flatten(),Y.flatten()]).T\n",
    "Xgrid = np.hstack([Xgrid,Z])\n",
    "\n",
    "n_test, dim = Xgrid.shape\n",
    "uest = tf.zeros((n_test,1),dtype=DTYPE)\n",
    "\n",
    "b_size = 512\n",
    "mc_samples=2000\n",
    "\n",
    "Xgrid = tf.convert_to_tensor(Xgrid, dtype=DTYPE)\n",
    "@tf.function\n",
    "def mc_step(y):\n",
    "    Xi = tf.random.normal(shape=(b_size, dim), dtype=DTYPE)\n",
    "    upd = tf.exp( (mu- tf.square(sigma)/2) * T + sigma * tf.sqrt(T) * Xi)\n",
    "    XT = tf.reshape(Xgrid, shape=[(ngrid+1)**2,dim,1])*tf.transpose(upd)\n",
    "    tmp = tf.reduce_sum(tf.reshape(fun_g(XT),[(ngrid+1)**2,b_size]), axis=1,keepdims=True)/(b_size*mc_samples)\n",
    "    return y + tmp\n",
    "\n",
    "#mc_step(uest)\n",
    "for i in range(mc_samples):\n",
    "    if i%100==0:\n",
    "        print(i,'/',mc_samples)\n",
    "\n",
    "    uest=mc_step(uest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "t4Bv1egQ0QMn",
    "outputId": "6f07cbd1-297e-461a-8793-2e0547f2a7d1"
   },
   "outputs": [],
   "source": [
    "# Relative error\n",
    "urel =tf.abs(model(Xgrid,training=False) -  uest)/uest\n",
    "\n",
    "Urel = urel.numpy().reshape(ngrid+1,ngrid+1)\n",
    "# Slice of solution\n",
    "U = model(Xgrid,training=False).numpy().reshape(ngrid+1,ngrid+1)\n",
    "\n",
    "# Plot solution of heat equation\n",
    "fig = plt.figure(figsize=(11,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X,Y,U,cmap='viridis')\n",
    "ax.view_init(45,210)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "#plt.savefig('Sol_OptionPricing.pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Plot relative error of heat equation\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X,Y,Urel,cmap='viridis')\n",
    "ax.view_init(45,210)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "#plt.savefig('RelError_OptionPricing.pdf', bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Feynman_Kac_Solver.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
